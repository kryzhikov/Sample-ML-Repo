{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3onVy6UMn3g"
      },
      "source": [
        "# Обработка текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKbHY4AZMn3k"
      },
      "source": [
        "##  Предобработка текстов "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn5Pw7USMn3k"
      },
      "source": [
        "Правильная предобработка текста позволяет добиться:\n",
        "* улучшения получаемых результатов\n",
        "* ускорения экспериментов\n",
        "* воспроизводимости экспериментов\n",
        "* удобной интерпретации и презентации результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jZ_8F3rMn3l"
      },
      "source": [
        "### Наивные методы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cycxLrmAMn3l",
        "outputId": "2452c785-daf9-4aa6-be77-3d64d23c0276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' «тинькофф банк» — российский коммерческий банк, сфокусированный полностью на дистанционном обслуживании\\n, не имеющий розничных отделений. штаб-квартира банка расположена в москве.'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = ''' «Тинькофф Банк» — российский коммерческий банк, сфокусированный полностью на дистанционном обслуживании\n",
        ", не имеющий розничных отделений. Штаб-квартира банка расположена в Москве.'''\n",
        "sent.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ON7a51EiMn3n",
        "outputId": "66132fbb-faf8-4545-c0ea-3a4ea5e45f77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '«Тинькофф',\n",
              " 'Банк»',\n",
              " '—',\n",
              " 'российский',\n",
              " 'коммерческий',\n",
              " 'банк,',\n",
              " 'сфокусированный',\n",
              " 'полностью',\n",
              " 'на',\n",
              " 'дистанционном',\n",
              " 'обслуживании\\n,',\n",
              " 'не',\n",
              " 'имеющий',\n",
              " 'розничных',\n",
              " 'отделений.',\n",
              " 'Штаб-квартира',\n",
              " 'банка',\n",
              " 'расположена',\n",
              " 'в',\n",
              " 'Москве.']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent.split(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1aaNJKwMn3o"
      },
      "source": [
        "### Регулярные выражение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF6sl9rvMn3o"
      },
      "source": [
        "Регулярное выражение — это последовательность символов, используемая для поиска и замены текста в строке или файле"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjc8evgIMn3p"
      },
      "source": [
        "* поиска в строке;\n",
        "* разбиения строки на подстроки;\n",
        "* замены части строки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7ldDPUJrMn3p",
        "outputId": "19280c4b-250b-4511-8baa-3b084fe73214"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'CheatSheet.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/2692633790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CheatSheet.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CheatSheet.png'"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='CheatSheet.png') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SSbj03FcMn3q"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hDvO-exvMn3q"
      },
      "outputs": [],
      "source": [
        "result = re.match(r'Tinkoff', ' junior Tinkoff the best')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z-bTYGQXMn3r",
        "outputId": "c2170be9-89d5-427d-80b8-c97b5f131398"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 7), match='Tinkoff'>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = re.search(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w7_mkhnSMn3r",
        "outputId": "19cb0f70-ef6f-479e-adfd-09666069a80e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tinkoff', 'Tinkoff']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = re.findall(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eAEOpABiMn3s",
        "outputId": "1227eeb0-da06-42ba-8d6d-930f68cd6095"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tinkoff Junior ', ' Best Team Tinkoff Bank']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = re.split(r'the', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J9WKCrJQMn3s",
        "outputId": "67343e6c-01bc-4993-ba56-cef30ee2e5c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tinkoff Junior the Best Team Tinkoff World'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = re.sub(r'Bank', 'World', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vLaJZJBMn3s"
      },
      "source": [
        "можно создавать паттерны"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MGNPN32YMn3t",
        "outputId": "3b790ee2-57f8-4512-b6df-5af1626402f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tinkoff', 'Tinkoff']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = re.compile('Tinkoff')\n",
        "result = pattern.findall('Tinkoff Junior the Best Team Tinkoff Bank')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BZpdMM_8Mn3t",
        "outputId": "1eaf434b-dc82-44e0-9522-7b0918ea6aeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "line = 'asdf fjdk;afed,fjek,asdf,foo' \n",
        "result = re.split(r'[;,\\s]', line)\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-SU9OY8GMn3t"
      },
      "outputs": [],
      "source": [
        "# TODO: Оставить в тексте только русские буквы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rfg_UtVCMn3u",
        "outputId": "c7dc47d0-aa75-498a-e655-930cdcfb7e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ночь закрытых дверей Штабквартира   ноября\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "sent = '''Ночь закрытых дверей!!! Штаб-квартира Tinkoff.ru 27 ноября'''\n",
        "expr = '[a-zA-Z0-9!.-]'\n",
        "parser=re.compile(expr)\n",
        "tmp_string = re.sub(expr, '', sent)\n",
        "print(tmp_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sSAa3uZUMn3u"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ночь закрытых дверей Штабквартира ноября'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub(r'\\s+',r' ',tmp_string )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PNQ2ALTuMn3u"
      },
      "outputs": [],
      "source": [
        "# TODO: Извлечь домены, заменить все домены на tinkoff.ru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W7qyCj2MMn3v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('gmail', 'com'), ('test', 'in'), ('analyticsvidhya', 'com'), ('rest', 'biz')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = re.findall(r'@(\\w+).(\\w+)', 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIGMbbC9Mn3v"
      },
      "source": [
        "Больше примеров регулярных выражений: https://regex101.com/r/nG1gU7/27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWnlt28nMn3v"
      },
      "source": [
        "### Спелл чекеры - проверка правописания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHQIsx9Mn3v"
      },
      "source": [
        "**Левенштейн**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwdAfG0pMn3v"
      },
      "source": [
        "Расстояние Левенштейна (также редакционное расстояние или дистанция редактирования) между двумя строками в теории информации и компьютерной лингвистике — это минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KXWB-I1MMn3v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m945.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /opt/homebrew/lib/python3.9/site-packages (from python-Levenshtein) (60.10.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp39-cp39-macosx_12_0_arm64.whl size=75417 sha256=4683a8b04d7da36935b52969e715e26465f83a32f46bc2fbe05b3009916fe75f\n",
            "  Stored in directory: /Users/artyom/Library/Caches/pip/wheels/46/4a/6c/164a1d9dd67c82d208f19d869ad0a517a0c5a6117f608c53e6\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed python-Levenshtein-0.12.2\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-7DR4wVmMn3w"
      },
      "outputs": [],
      "source": [
        "import Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wl7ii6DMn3x"
      },
      "source": [
        "https://pypi.org/project/python-Levenshtein/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zzwZrmVQMn3y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Levenshtein.distance('Банк', 'ит-компания')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqJutyO9Mn3y"
      },
      "source": [
        "### Уменьшение словаря\n",
        "* Плохие слова:\n",
        "* Слишком частые \n",
        "  * русский язык: и, но, я, ты, ... \n",
        "  * английский язык: a, the, I, ... \n",
        "  * специфичные для коллекции: \"сообщать\" в новостях\n",
        "* Слишком редкие\n",
        "* Стоп-слова \n",
        "  * Предлоги, междометия, частицы, цифры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6UXdyJ8Mn3y"
      },
      "source": [
        "**NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDFp40gMn3y"
      },
      "source": [
        "https://www.nltk.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n_Ypg8P7Mn3y",
        "outputId": "b9ea09a2-41e4-43c2-ed83-0b9aec776e2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/artyom/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IZxYHUruMn3z",
        "outputId": "c9a24512-e8b5-40d7-80f3-b98dbfa68fab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['up', \"it's\", 'but', 'any', 'who', 'themselves']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw_eng = set(stopwords.words('english'))\n",
        "list(sw_eng)[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AWO-wVR1Mn3z",
        "outputId": "ffba845f-cb7f-4462-f8a7-768ab70c6aa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "151"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw_ru = set(stopwords.words('russian'))\n",
        "len(sw_ru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RHj-uKxWMn3z",
        "outputId": "43153001-7aa1-4f65-a49c-06521dd70932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "До 90 слов\n",
            "После 66 слов\n"
          ]
        }
      ],
      "source": [
        "sent = 'Наступило молчание. Графиня глядела на гостью, приятно улыбаясь, впрочем, не скрывая \\\n",
        "того, что не огорчится теперь нисколько, если гостья поднимется и уедет.\\\n",
        "Дочь гостьи уже оправляла платье, вопросительно глядя на мать, как вдруг из\\\n",
        "соседней комнаты послышался бег к двери нескольких мужских и женских ног,\\\n",
        "грохот зацепленного и поваленного стула, и в комнату вбежала тринадцатилетняя девочка,\\\n",
        "запахнув что-то короткою кисейною юбкою, и остановилась посередине комнаты. Очевидно было,\\\n",
        "она нечаянно, с нерассчитанного бега, заскочила так далеко. В дверях в ту же минуту показались\\\n",
        "студент с малиновым воротником, гвардейский офицер, пятнадцатилетняя девочка и толстый румяный\\\n",
        "мальчик в детской курточке.'\n",
        "\n",
        "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
        "print('До {} слов'.format(len(sent.split())))\n",
        "print('После {} слов'.format(len(clean_sent.split())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bDgaPSsFMn3z",
        "outputId": "b07f28b2-c479-48be-d2fd-b9634c6e09ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Наступило молчание. Графиня глядела гостью, приятно улыбаясь, впрочем, скрывая того, огорчится нисколько, гостья поднимется уедет.Дочь гостьи оправляла платье, вопросительно глядя мать, изсоседней комнаты послышался бег двери нескольких мужских женских ног,грохот зацепленного поваленного стула, комнату вбежала тринадцатилетняя девочка,запахнув что-то короткою кисейною юбкою, остановилась посередине комнаты. Очевидно было,она нечаянно, нерассчитанного бега, заскочила далеко. В дверях ту минуту показалисьстудент малиновым воротником, гвардейский офицер, пятнадцатилетняя девочка толстый румяныймальчик детской курточке.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w3IvtJCLMn3z",
        "outputId": "af628aad-8c6c-4c76-e8f2-e3b163df41de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = 'быть или не быть'\n",
        "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
        "clean_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwgK_mW2Mn30"
      },
      "source": [
        "### Токенизация\n",
        "разделение на токены, элементарные единицы текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-uBU8vrcMn30",
        "outputId": "b74f6e31-cec0-446f-cdb6-91a48e372e0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Привет,',\n",
              " 'какая',\n",
              " 'у',\n",
              " 'меня',\n",
              " 'полная',\n",
              " 'сумма',\n",
              " 'задолженности',\n",
              " 'по',\n",
              " 'кредитной',\n",
              " 'карте']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "light_string = 'Привет, какая у меня полная сумма задолженности по кредитной карте'\n",
        "light_string.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cdLBsApTMn30",
        "outputId": "6a8721cd-1b9c-44dc-c374-7d6cd9469a99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ой,',\n",
              " 'у',\n",
              " 'вас',\n",
              " 'несколько',\n",
              " 'кредитных',\n",
              " 'карт,',\n",
              " 'выберите,',\n",
              " 'пожалуйста,',\n",
              " 'одну',\n",
              " 'и',\n",
              " 'введите',\n",
              " 'ее',\n",
              " 'номер']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hard_string = 'Ой, у вас несколько кредитных карт, выберите, пожалуйста, одну и введите ее номер'\n",
        "hard_string.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uNs7ar6OMn30",
        "outputId": "30e0af5f-801a-403b-fcea-81de04c84406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Привет', 'Ты', 'видел', 'мр.Смита', 'сегодня', 'утром']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "hard_string = 'Привет! Ты видел мр.Смита сегодня утром?'\n",
        "expr = r'[^(\\w.\\w)\\w\\s]'\n",
        "parser=re.compile(expr)\n",
        "tmp_string = parser.sub(r'', hard_string)\n",
        "print(tmp_string.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bZIDAnLZMn30",
        "outputId": "5193bfa6-d4d6-4896-9736-2762508c6b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Привет', ' Ты видел мр', 'Смита сегодня утром', '']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
        "tmp_string = re.split(r'[!.?]', hard_string)\n",
        "print(tmp_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ukSH4Y73Mn31",
        "outputId": "7d1d2419-12fe-4169-a042-e8db3723a8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Привет.', 'Ты видел мр.Смита сегодня утром?']\n"
          ]
        }
      ],
      "source": [
        "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
        "exp = r'(?<!\\w\\.\\w.)(?<![А-Я][а-я]\\.)(?<=\\.|\\?)\\s'\n",
        "tmp_string = re.split(exp, hard_string)\n",
        "print(tmp_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvjdsUV9Mn31"
      },
      "source": [
        "### Нормализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6-CYQ9bMn31"
      },
      "source": [
        "**Стемминг** - нормализация слов путем отбрасывания окончаний (согласно правилам, основанным на грамматике языка)\n",
        "* Стеммеры (nltk)\n",
        "    * Porter stemmer\n",
        "    * Snowball stemmer\n",
        "    * Lancaster stemmer\n",
        "    * MyStem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CzYDDmE5Mn31",
        "outputId": "16b003cd-affe-4bbb-b8f5-3aebd7fa30d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "georg admit the talk happen\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "sent = 'George admitted the talks happened'\n",
        "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8-Cg4RJ8Mn31",
        "outputId": "285f9c9e-29fd-404e-db1f-8cf1400a6462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "write wrote written\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "sent = 'write wrote written'\n",
        "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "auEZmbWgMn31",
        "outputId": "deb92f64-141d-4534-be49-73ccd7eb931c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "опрошен счита налог необходим\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='russian')\n",
        "sent = 'Опрошенные считают налоги необходимыми'\n",
        "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3ySNHmK7Mn32",
        "outputId": "332c8dc1-074c-4c4f-e925-c37b538ce30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "пол пол полк полк\n"
          ]
        }
      ],
      "source": [
        "sent = 'поле пол полка полк'\n",
        "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Q22UtfYhMn32",
        "outputId": "5df49dc7-7c0a-4fac-ebe3-9fb8bf35c7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "крут крут крут\n"
          ]
        }
      ],
      "source": [
        "sent = 'крутой крутейший крутить'\n",
        "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2qwi18Mn32"
      },
      "source": [
        "**Лемматизация** - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVB19JsrMn32"
      },
      "source": [
        "Лемматизаторы:\n",
        "* pymorphy2 (язык русский, украинский)\n",
        "* mystem3 (язык русский)\n",
        "* Wordnet Lemmatizer (NLTK, язык английский, требует POS метку)\n",
        "* Metaphraz (язык русский)\n",
        "* Coda/Cadenza (языки русский и английский)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du066rVWMn32"
      },
      "source": [
        "Лемматизатор на самом деле довольно сложно устроены, им нужны теги частей речи (POS).\n",
        "\n",
        "По умолчанию функция WordNetLemmatizer.lemmatize () будет считать, что это слово является существительным, если на входе не обнаружен тег POS.\n",
        "\n",
        "Сначала вам понадобится функция pos_tag, чтобы пометить предложение и использовать тег, чтобы преобразовать его в теги WordNet, а затем передать его в WordNetLemmatizer.\n",
        "\n",
        "Примечание. Лемматизация не будет работать только на одиночных словах без контекста или знании своего тега POS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9qmv2ChxMn32"
      },
      "outputs": [],
      "source": [
        "from nltk import wordnet, pos_tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    my_switch = {\n",
        "        'J': wordnet.wordnet.ADJ,\n",
        "        'V': wordnet.wordnet.VERB,\n",
        "        'N': wordnet.wordnet.NOUN,\n",
        "        'R': wordnet.wordnet.ADV,\n",
        "    }\n",
        "    for key, item in my_switch.items():\n",
        "        if treebank_tag.startswith(key):\n",
        "            return item\n",
        "    return wordnet.wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iUP0VXRhMn32",
        "outputId": "e20ae99f-7255-4bc2-976b-0e206fc3035e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/artyom/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UE6VFlDIMn33",
        "outputId": "12ff1d0f-21fb-4a91-d33e-3d96fa023ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('George', 'NNP'), ('admitted', 'VBD'), ('the', 'DT'), ('talks', 'NNS'), ('happened', 'VBD')]\n"
          ]
        }
      ],
      "source": [
        "sent = 'George admitted the talks happened'.split()\n",
        "pos_tagged = pos_tag(sent)\n",
        "print(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "y4KxxHBoMn33",
        "outputId": "86cd39b3-bf5c-431a-b9b4-e8ada0a7f3df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/artyom/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "r1P38WpEMn33",
        "outputId": "a87dd734-bfc3-4b33-86cc-b7929508e7b7"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'nltk.stem.wordnet' has no attribute 'ADJ'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/609892621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/609892621.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/3598986044.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(treebank_tag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreebank_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     my_switch = {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m'J'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m'V'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.stem.wordnet' has no attribute 'ADJ'"
          ]
        }
      ],
      "source": [
        "print([get_wordnet_pos(tag) for word, tag in pos_tagged])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENKrRu7aMn33"
      },
      "outputs": [],
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "def my_lemmatizer(sent):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenized_sent = sent.split()\n",
        "    pos_tagged = [(word, get_wordnet_pos(tag))\n",
        "                 for word, tag in pos_tag(tokenized_sent)]\n",
        "    return ' '.join([lemmatizer.lemmatize(word, tag)\n",
        "                    for word, tag in pos_tagged])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsFV671AMn33",
        "outputId": "b5bbf3e6-0707-4a77-8257-e3f3e37598d7"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'nltk.stem.wordnet' has no attribute 'wordnet'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/2379238488.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'George admitted the talks happened'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_lemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/1887298887.py\u001b[0m in \u001b[0;36mmy_lemmatizer\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     pos_tagged = [(word, get_wordnet_pos(tag))\n\u001b[0m\u001b[1;32m      6\u001b[0m                  for word, tag in pos_tag(tokenized_sent)]\n\u001b[1;32m      7\u001b[0m     return ' '.join([lemmatizer.lemmatize(word, tag)\n",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/1887298887.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     pos_tagged = [(word, get_wordnet_pos(tag))\n\u001b[0m\u001b[1;32m      6\u001b[0m                  for word, tag in pos_tag(tokenized_sent)]\n\u001b[1;32m      7\u001b[0m     return ' '.join([lemmatizer.lemmatize(word, tag)\n",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/2513557826.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(treebank_tag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreebank_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     my_switch = {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m'J'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m'V'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.stem.wordnet' has no attribute 'wordnet'"
          ]
        }
      ],
      "source": [
        "sent = 'George admitted the talks happened'\n",
        "my_lemmatizer(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "F-fsDsrFMn33",
        "outputId": "c5844460-a67d-4780-f02b-93e9979717f8"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'nltk.stem.wordnet' has no attribute 'ADJ'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/1836273847.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'write wrote written'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_lemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/1887298887.py\u001b[0m in \u001b[0;36mmy_lemmatizer\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     pos_tagged = [(word, get_wordnet_pos(tag))\n\u001b[0m\u001b[1;32m      6\u001b[0m                  for word, tag in pos_tag(tokenized_sent)]\n\u001b[1;32m      7\u001b[0m     return ' '.join([lemmatizer.lemmatize(word, tag)\n",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/1887298887.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     pos_tagged = [(word, get_wordnet_pos(tag))\n\u001b[0m\u001b[1;32m      6\u001b[0m                  for word, tag in pos_tag(tokenized_sent)]\n\u001b[1;32m      7\u001b[0m     return ' '.join([lemmatizer.lemmatize(word, tag)\n",
            "\u001b[0;32m/var/folders/9j/cllsf0815jl9bm0yjv_8cm400000gn/T/ipykernel_19601/3598986044.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(treebank_tag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreebank_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     my_switch = {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m'J'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m'V'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.stem.wordnet' has no attribute 'ADJ'"
          ]
        }
      ],
      "source": [
        "sent = 'write wrote written'\n",
        "my_lemmatizer(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JVcpOWvmMn34",
        "outputId": "a7f20c7d-be7e-4b9a-ef4f-fe8ff1325374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=89d7f1d2c7f141ab784a956b12e63b128f70036ea5f1ab87dd1ee2c497ce7ca0\n",
            "  Stored in directory: /Users/artyom/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EojRnRsGMn34"
      },
      "outputs": [],
      "source": [
        "import pymorphy2\n",
        "def my_lemmatizer_ru(sent):\n",
        "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
        "    tokenized_sent = sent.split()\n",
        "    return ' '.join([lemmatizer.parse(word)[0].normal_form\n",
        "                    for word in tokenized_sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "LflptV1XMn34",
        "outputId": "15391744-64b7-4a0f-84d0-5e27a43fc315"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'опросить считать налог необходимый'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = 'Опрошенные считают налоги необходимыми'\n",
        "my_lemmatizer_ru(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "SM37A2reMn34",
        "outputId": "a42558b4-7d5d-4e91-c18f-fd4ce28799ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'выйти в поле с конь'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = 'Выйду в поле с конем'\n",
        "my_lemmatizer_ru(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "gGbC8GsaMn34",
        "outputId": "a9710b7f-ef05-476e-b0c7-f22cc4fb06a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "крутой крутый крутить\n"
          ]
        }
      ],
      "source": [
        "sent = 'крутой крутейший крутить'\n",
        "print(my_lemmatizer_ru(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuwGFsx3Mn34"
      },
      "source": [
        "*Стемминг *\n",
        "* Плохо работает для русского языка\n",
        "* Нормально работает для английского\n",
        "* Повышает качество модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARg0cgGHMn35"
      },
      "source": [
        "*Лемматизация*\n",
        "* Лучше стемминга для русского языка\n",
        "* Хорошо работает для английского языка\n",
        "* Повышает качество модели\n",
        "* Гораздо медленнее чем стемминг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfSM0YPfMn35"
      },
      "source": [
        "## Представление текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvPk75fRMn35"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "Представление словаря в виде бинарных векторов, у которых все значения равны 0, кроме одного, отвечающего за соответствующее слово"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "8AZKEo1bMn35"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "corpus = ['Кредитная Дебетовая', 'Дебетовая', 'All Airlines', 'Bravo', 'All games']\n",
        "label_encoder = LabelEncoder()\n",
        "corpus_encoded = label_encoder.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_D5zqHMLMn35",
        "outputId": "4e1a7811-9965-400e-bdfc-d84eebf78175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['All Airlines', 'All games', 'Bravo', 'Дебетовая',\n",
              "       'Кредитная Дебетовая'], dtype='<U19')"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoder.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dlw9WQ6KMn35",
        "outputId": "46c980f8-7e1f-43de-8f5f-04a4f9e6d3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "corpus_encoded = corpus_encoded.reshape(len(corpus_encoded), 1)\n",
        "corpus_onehot_encoded = onehot_encoder.fit_transform(corpus_encoded)\n",
        "print(corpus_onehot_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uUqKzygMn35"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "IJlzJjbaMn35",
        "outputId": "1446ea81-37ff-4bbf-c345-afa0174d9897"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "pOomgK_dMn36",
        "outputId": "a1f3193f-e499-47b8-a1b8-396a455ed64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'this': 8, 'is': 2, 'the': 6, 'first': 1, 'text': 5, 'second': 4, 'and': 0, 'third': 7, 'one': 3}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    'This is the first text text text.',\n",
        "    'This is the second second text.',\n",
        "    'And the third one.',\n",
        "    'Is this the first text?',\n",
        "]\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "FghlzaGOMn36",
        "outputId": "52fe544d-c7f2-4899-c4ff-f4d3bd0fc209"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0, 1, 1, 0, 0, 3, 1, 0, 1],\n",
              "        [0, 0, 1, 0, 2, 1, 1, 0, 1],\n",
              "        [1, 0, 0, 1, 0, 0, 1, 1, 0],\n",
              "        [0, 1, 1, 0, 0, 1, 1, 0, 1]])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJTNCKSPMn36"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Q3xjpkGkMn36"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "idf_vectorizer = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Cvaa0ednMn36",
        "outputId": "a72e661c-0abd-4c98-926d-0b8993d33d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "Y = idf_vectorizer.fit_transform(corpus)\n",
        "print(idf_vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "lLGE0zjlMn36",
        "outputId": "56c656a3-10dc-4837-b18a-15cd48941d63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "         0.        , 0.35872874, 0.        , 0.43877674],\n",
              "        [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
              "         0.85322574, 0.22262429, 0.        , 0.27230147],\n",
              "        [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
              "         0.        , 0.28847675, 0.55280532, 0.        ],\n",
              "        [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "         0.        , 0.35872874, 0.        , 0.43877674]])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URgBobDUMn37"
      },
      "source": [
        "## Задача"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "O9zRVZSQMn37",
        "outputId": "0419add2-24fc-4821-d3ef-af6e4d0e9fce"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train',\n",
        "                                  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "                                 categories=categories, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "cOgjYq63Mn37",
        "outputId": "2172ad27-58e9-487a-f506-3af5e3068b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "Organization: The City University\n",
            "Lines: 14\n",
            "\n",
            "Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do the same, converting to HPGL (HP plotter) files.\n",
            "\n",
            "Please email any response.\n",
            "\n",
            "Is this the correct group?\n",
            "\n",
            "Thanks in advance.  Michael.\n",
            "-- \n",
            "Michael Collier (Programmer)                 The Computer Unit,\n",
            "Email: M.P.Collier@uk.ac.city                The City University,\n",
            "Tel: 071 477-8000 x3769                      London,\n",
            "Fax: 071 477-8565                            EC1V 0HB.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "wnSkCrtaMn37",
        "outputId": "9e1aa30d-a831-4313-c705-1db0e56410ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "comp.graphics\n"
          ]
        }
      ],
      "source": [
        "print(twenty_train.target_names[twenty_train.target[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "xECDwOYOMn37",
        "outputId": "0cf18847-7566-482a-8af3-de9aa957d9c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2257"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(twenty_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "1mVYcul0Mn37",
        "outputId": "c5908878-1ef6-420e-e52e-5f6e83761f44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twenty_train.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "mAwUl9tjMn37",
        "outputId": "bc618174-3873-41db-f1a6-2b32013de29a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twenty_train.target[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ns1OBqIjMn38",
        "outputId": "8ba7e52d-fff4-4dc8-ecae-b60d30373f1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "R4uPEKbzMn38",
        "outputId": "dc106a34-ada9-4a25-97e3-b9b83111646e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1502, 35788)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_counts = count_vect.transform(twenty_test.data)\n",
        "X_test_counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4NmnIqSiMn38"
      },
      "outputs": [],
      "source": [
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "JjuU5Ix9Mn38"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "svm = SGDClassifier()\n",
        "svm.fit(X_train_counts, twenty_train.target)\n",
        "predicted = svm.predict(X_test_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ubRKiLhHMn38",
        "outputId": "55c35b4e-f2d4-41a0-9ddc-c303f38d637e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.84      0.81      0.82       319\n",
            "         comp.graphics       0.86      0.95      0.90       389\n",
            "               sci.med       0.93      0.82      0.87       396\n",
            "soc.religion.christian       0.87      0.92      0.89       398\n",
            "\n",
            "              accuracy                           0.88      1502\n",
            "             macro avg       0.88      0.87      0.87      1502\n",
            "          weighted avg       0.88      0.88      0.88      1502\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted,\n",
        "                                    target_names=twenty_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "GGdcKaVhMn38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.95      0.85      0.90       319\n",
            "         comp.graphics       0.90      0.96      0.93       389\n",
            "               sci.med       0.96      0.91      0.93       396\n",
            "soc.religion.christian       0.91      0.96      0.93       398\n",
            "\n",
            "              accuracy                           0.93      1502\n",
            "             macro avg       0.93      0.92      0.92      1502\n",
            "          weighted avg       0.93      0.93      0.93      1502\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Самостоятельная работа: попробовать другие преобразования\n",
        " TfidfVectorizer\n",
        "tfid_vect = TfidfVectorizer()\n",
        "X_train_counts = tfid_vect.fit_transform(twenty_train.data)\n",
        "\n",
        "X_test_counts = tfid_vect.transform(twenty_test.data)\n",
        "\n",
        "svm = SGDClassifier()\n",
        "svm.fit(X_train_counts, twenty_train.target)\n",
        "predicted = svm.predict(X_test_counts)\n",
        "\n",
        "print(metrics.classification_report(twenty_test.target, predicted,\n",
        "                                    target_names=twenty_test.target_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "inclass_nlp.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
