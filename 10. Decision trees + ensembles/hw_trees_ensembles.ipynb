{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EADmNKm_ucFI"
      },
      "source": [
        "## Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wjurpt4KJmCh"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OKFSv2GpuUFDphO0r8LdM7bl6MAWwBfX' -O data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBAI5LS2xBSm"
      },
      "source": [
        "В этой домашней работе вы будете предсказывать стоимость домов по их характеристикам.\n",
        "\n",
        "Метрика качества: `RMSE`\n",
        "\n",
        "Оценивание:\n",
        "* Baseline - 2 балла\n",
        "* Feature Engineering - 2 балла\n",
        "* Model Selection - 3 балла\n",
        "* Ensemble v.1 - 3 балла\n",
        "* (*) Ensemble v.2 - дополнительно, 2 балла"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBZhr_dw6HAn"
      },
      "source": [
        "### Описание датасета\n",
        "\n",
        "Короткое описание данных:\n",
        "```\n",
        "price: sale price (this is the target variable)\n",
        "id: transaction id\n",
        "timestamp: date of transaction\n",
        "full_sq: total area in square meters, including loggias, balconies and other non-residential areas\n",
        "life_sq: living area in square meters, excluding loggias, balconies and other non-residential areas\n",
        "floor: for apartments, floor of the building\n",
        "max_floor: number of floors in the building\n",
        "material: wall material\n",
        "build_year: year built\n",
        "num_room: number of living rooms\n",
        "kitch_sq: kitchen area\n",
        "state: apartment condition\n",
        "product_type: owner-occupier purchase or investment\n",
        "sub_area: name of the district\n",
        "\n",
        "The dataset also includes a collection of features about each property's surrounding neighbourhood, and some features that are constant across each sub area (known as a Raion). Most of the feature names are self explanatory, with the following notes. See below for a complete list.\n",
        "\n",
        "full_all: subarea population\n",
        "male_f, female_f: subarea population by gender\n",
        "young_*: population younger than working age\n",
        "work_*: working-age population\n",
        "ekder_*: retirement-age population\n",
        "n_m_{all|male|female}: population between n and m years old\n",
        "build_count_*: buildings in the subarea by construction type or year\n",
        "x_count_500: the number of x within 500m of the property\n",
        "x_part_500: the share of x within 500m of the property\n",
        "_sqm_: square meters\n",
        "cafe_count_d_price_p: number of cafes within d meters of the property that have an average bill under p RUB\n",
        "trc_: shopping malls\n",
        "prom_: industrial zones\n",
        "green_: green zones\n",
        "metro_: subway\n",
        "_avto_: distances by car\n",
        "mkad_: Moscow Circle Auto Road\n",
        "ttk_: Third Transport Ring\n",
        "sadovoe_: Garden Ring\n",
        "bulvar_ring_: Boulevard Ring\n",
        "kremlin_: City center\n",
        "zd_vokzaly_: Train station\n",
        "oil_chemistry_: Dirty industry\n",
        "ts_: Power plant\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOPtM_06uyqv"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9TpHMcn3u0MV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import clone\n",
        "\n",
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-TLkVG3EwJzj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data.csv\", parse_dates=[\"timestamp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>full_sq</th>\n",
              "      <th>life_sq</th>\n",
              "      <th>floor</th>\n",
              "      <th>max_floor</th>\n",
              "      <th>material</th>\n",
              "      <th>build_year</th>\n",
              "      <th>num_room</th>\n",
              "      <th>kitch_sq</th>\n",
              "      <th>...</th>\n",
              "      <th>cafe_count_5000_price_2500</th>\n",
              "      <th>cafe_count_5000_price_4000</th>\n",
              "      <th>cafe_count_5000_price_high</th>\n",
              "      <th>big_church_count_5000</th>\n",
              "      <th>church_count_5000</th>\n",
              "      <th>mosque_count_5000</th>\n",
              "      <th>leisure_count_5000</th>\n",
              "      <th>sport_count_5000</th>\n",
              "      <th>market_count_5000</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-12-26</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>75</td>\n",
              "      <td>10</td>\n",
              "      <td>15318960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-10-04</td>\n",
              "      <td>64</td>\n",
              "      <td>64.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2014-02-05</td>\n",
              "      <td>83</td>\n",
              "      <td>44.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1985.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>17000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2012-07-26</td>\n",
              "      <td>71</td>\n",
              "      <td>49.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2014-10-29</td>\n",
              "      <td>60</td>\n",
              "      <td>42.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1970.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>5</td>\n",
              "      <td>7900000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 292 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  timestamp  full_sq  life_sq  floor  max_floor  material  build_year  \\\n",
              "0   0 2014-12-26        1      1.0    1.0        1.0       1.0         1.0   \n",
              "1   1 2012-10-04       64     64.0   16.0        NaN       NaN         NaN   \n",
              "2   2 2014-02-05       83     44.0    9.0       17.0       1.0      1985.0   \n",
              "3   3 2012-07-26       71     49.0    2.0        NaN       NaN         NaN   \n",
              "4   4 2014-10-29       60     42.0    9.0        9.0       1.0      1970.0   \n",
              "\n",
              "   num_room  kitch_sq  ...  cafe_count_5000_price_2500  \\\n",
              "0       1.0       1.0  ...                          36   \n",
              "1       NaN       NaN  ...                           2   \n",
              "2       3.0      10.0  ...                          13   \n",
              "3       NaN       NaN  ...                           0   \n",
              "4       3.0       6.0  ...                           3   \n",
              "\n",
              "  cafe_count_5000_price_4000 cafe_count_5000_price_high  \\\n",
              "0                          7                          2   \n",
              "1                          2                          0   \n",
              "2                          6                          1   \n",
              "3                          0                          0   \n",
              "4                          1                          0   \n",
              "\n",
              "   big_church_count_5000  church_count_5000  mosque_count_5000  \\\n",
              "0                     15                 33                  1   \n",
              "1                      0                 13                  1   \n",
              "2                      8                 18                  0   \n",
              "3                      1                  3                  0   \n",
              "4                      5                  8                  0   \n",
              "\n",
              "   leisure_count_5000  sport_count_5000  market_count_5000     price  \n",
              "0                  12                75                 10  15318960  \n",
              "1                   0                 6                  1   6080000  \n",
              "2                   1                52                  0  17000000  \n",
              "3                   2                 8                  2    990000  \n",
              "4                   1                34                  5   7900000  \n",
              "\n",
              "[5 rows x 292 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20000 entries, 0 to 19999\n",
            "Columns: 292 entries, id to price\n",
            "dtypes: datetime64[ns](1), float64(119), int64(157), object(15)\n",
            "memory usage: 44.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjwH9_mx3fZ"
      },
      "source": [
        "Разделите имеющиеся у вас данные на обучающую и тестовую выборки. В качестве обучающей выборки возьмите первые 80% данных, последние 20% - тестовая выборка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hwfNR565wMKN"
      },
      "outputs": [],
      "source": [
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL4dJJwZP86w"
      },
      "source": [
        "Возможно в ваших моделях вам придется указывать, какие колонки являются категориальными (например, в бустингах). Для упрощения предлагается разделить колонки по следующему принципу:\n",
        "```\n",
        "drop_columns = [\n",
        "    'id',           # May leak information\n",
        "    'timestamp',    # May leak information\n",
        "]\n",
        "cat_columns = [\n",
        "    'product_type',              #\n",
        "    'material',                  # Material of the wall\n",
        "    'state',                     # Satisfaction level\n",
        "    'sub_area',                  # District name\n",
        "    'culture_objects_top_25',    #\n",
        "    'thermal_power_plant_raion', #\n",
        "    'incineration_raion',        #\n",
        "    'oil_chemistry_raion',       #\n",
        "    'radiation_raion',           #\n",
        "    'railroad_terminal_raion',   #\n",
        "    'big_market_raion',          #\n",
        "    'nuclear_reactor_raion',     #\n",
        "    'detention_facility_raion',  #\n",
        "    'ID_metro',                  #\n",
        "    'ID_railroad_station_walk',  #\n",
        "    'ID_railroad_station_avto',  #\n",
        "    'water_1line',               #\n",
        "    'ID_big_road1',              #\n",
        "    'big_road1_1line',           #\n",
        "    'ID_big_road2',              #\n",
        "    'railroad_1line',            #\n",
        "    'ID_railroad_terminal',      #\n",
        "    'ID_bus_terminal',           #\n",
        "    'ecology',                   #\n",
        "]\n",
        "num_columns = list(set(df.columns).difference(set(cat_columns + drop_columns)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiOby6hNu92W"
      },
      "source": [
        "### Baseline (2 балла)\n",
        "\n",
        "В качестве Baseline обучите `DecisionTreeRegressor` из `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gJIDf3kn1Wfv"
      },
      "outputs": [],
      "source": [
        "drop_columns = [\n",
        "    'id',           # May leak information\n",
        "    'timestamp',    # May leak information\n",
        "]\n",
        "cat_columns = [\n",
        "    'product_type',              #\n",
        "    'material',                  # Material of the wall\n",
        "    'state',                     # Satisfaction level\n",
        "    'sub_area',                  # District name\n",
        "    'culture_objects_top_25',    #\n",
        "    'thermal_power_plant_raion', #\n",
        "    'incineration_raion',        #\n",
        "    'oil_chemistry_raion',       #\n",
        "    'radiation_raion',           #\n",
        "    'railroad_terminal_raion',   #\n",
        "    'big_market_raion',          #\n",
        "    'nuclear_reactor_raion',     #\n",
        "    'detention_facility_raion',  #\n",
        "    'ID_metro',                  #\n",
        "    'ID_railroad_station_walk',  #\n",
        "    'ID_railroad_station_avto',  #\n",
        "    'water_1line',               #\n",
        "    'ID_big_road1',              #\n",
        "    'big_road1_1line',           #\n",
        "    'ID_big_road2',              #\n",
        "    'railroad_1line',            #\n",
        "    'ID_railroad_terminal',      #\n",
        "    'ID_bus_terminal',           #\n",
        "    'ecology',                   #\n",
        "]\n",
        "num_columns = list(set(df.columns).difference(set(cat_columns + drop_columns)))\n",
        "num_columns_train = copy.deepcopy(num_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_columns_train.remove(\"price\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для baseline модели не будем учитывать категориальные признаки. Вместо nan вставим медианное значение признака с помощью imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_num = X_train.drop(drop_columns + cat_columns, axis=1)\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "X_train_num = imputer.fit_transform(X_train_num)\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=42).fit(X_train_num, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUC7NFhw1X41"
      },
      "source": [
        "Проверьте качество на отложенной выборке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O_LCiQIN1bzD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of baseline solution = 4355980.7\n"
          ]
        }
      ],
      "source": [
        "X_test_num = X_test.drop(drop_columns + cat_columns, axis=1)\n",
        "X_test_num = imputer.transform(X_test_num)\n",
        "\n",
        "y_pred = dt_reg.predict(X_test_num)\n",
        "\n",
        "rmse_baseline = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f\"RMSE of baseline solution = {rmse_baseline:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bezBBI7u_oF"
      },
      "source": [
        "### Feature Engineering (2 балла)\n",
        "\n",
        "Часто улучшить модель можно с помощью аккуратного Feature Engineering.\n",
        "\n",
        "Добавим в модель дополнительные признаки:\n",
        "* \"Как часто в этот год и этот месяц появлились объявления\"\n",
        "* \"Как часто в этот год и эту неделю появлялись объявления\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_cJ6SBqUKC4C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shamil/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "month_year = (df.timestamp.dt.month + df.timestamp.dt.year * 100)\n",
        "month_year_cnt_map = month_year.value_counts().to_dict()\n",
        "df[\"month_year_cnt\"] = month_year.map(month_year_cnt_map)\n",
        "\n",
        "week_year = (df.timestamp.dt.weekofyear + df.timestamp.dt.year * 100)\n",
        "week_year_cnt_map = week_year.value_counts().to_dict()\n",
        "df[\"week_year_cnt\"] = week_year.map(week_year_cnt_map)\n",
        "num_columns_train.extend([\"month_year_cnt\", \"week_year_cnt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCajk45_KDSz"
      },
      "source": [
        "Добавьте следюущие дополнительные признаки:\n",
        "* Месяц (из колонки `timestamp`)\n",
        "* День недели (из колонки `timestamp`)\n",
        "* Отношение \"этаж / максимальный этаж в здании\" (колонки `floor` и `max_floor`)\n",
        "* Отношение \"площадь кухни / площадь квартиры\" (колонки `kitchen_sq` и `full_sq`)\n",
        "\n",
        "По желанию можно добавить и другие признаки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "У некоторых выборок пропущены некоторые признаки. Воспользуем imputer и вставим вместо пропущенных признаков медианное значение среди всех выборок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['max_floor'] = clone(median_imputer).fit_transform(df['max_floor'].to_numpy().reshape(-1,1))\n",
        "df['floor'] = clone(median_imputer).fit_transform(df['floor'].to_numpy().reshape(-1,1))\n",
        "df['kitch_sq'] = clone(median_imputer).fit_transform(df['kitch_sq'].to_numpy().reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dXZkoUL8Kve6"
      },
      "outputs": [],
      "source": [
        "df['month'] = df['timestamp'].dt.month\n",
        "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
        "df['rel_floor'] =  df['floor']/df['max_floor']\n",
        "df['rel_floor'] = df['rel_floor'].apply(lambda x: 0 if x==np.inf else x)\n",
        "\n",
        "df['rel_kitch_sq'] = df['kitch_sq']/df['full_sq']\n",
        "num_columns_train.extend([\"month\", \"dayofweek\", \"rel_floor\", \"rel_kitch_sq\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для алгоритмов, которые по умолчанию не умеют работать с  категориальными данными сделаем **OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество признаков в изначальной выборке = 296\n",
            "Количество признаков в новой выборке = 1071\n"
          ]
        }
      ],
      "source": [
        "df.drop(drop_columns, axis=1, inplace=True)\n",
        "print(f\"Количество признаков в изначальной выборке = {df.shape[1]}\")\n",
        "df_1hot = pd.get_dummies(df, columns=cat_columns)\n",
        "print(f\"Количество признаков в новой выборке = {df_1hot.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-jCowvWLiXZ"
      },
      "source": [
        "Разделите выборку на обучающую и тестовую еще раз (потому что дополнительные признаки созданы для исходной выборки)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0f-vO3LcLhdf"
      },
      "outputs": [],
      "source": [
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "X1hot = df_1hot.drop('price', axis=1)\n",
        "y1hot = df_1hot['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "X1hot_train, X1hot_test, y1hot_train, y1hot_test = train_test_split(X1hot, y1hot, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_cat_columns = list(set(df_1hot.columns).difference(set(num_columns_train + ['price'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "После разделения заменим NaN медианном значением признака "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_num = X_train.drop(cat_columns, axis=1)\n",
        "X_test_num = X_test.drop(cat_columns, axis=1)\n",
        "\n",
        "imputer_num = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "X_train_num = imputer_num.fit_transform(X_train_num)\n",
        "X_test_num = imputer_num.transform(X_test_num)\n",
        "\n",
        "imputer_1hot = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "X1hot_train = imputer_1hot.fit_transform(X1hot_train)\n",
        "X1hot_test = imputer_1hot.transform(X1hot_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Так как у нас стало много признаков, попробуем уменшить размерность использую алгортим основанный на деревьях и сравним предсказания моделей, обученных всех или уменьшенных признаках"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.feature_selection import SelectFromModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "rg = ExtraTreesRegressor(n_estimators=50, random_state=42)\n",
        "rg.fit(X1hot_train, y1hot_train)\n",
        "\n",
        "model = SelectFromModel(rg, prefit=True)\n",
        "X1hot_train_red = model.transform(X1hot_train)\n",
        "X1hot_test_red = model.transform(X1hot_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPtzkpFEvBw9"
      },
      "source": [
        "### Model Selection (3 балла)\n",
        "\n",
        "Посмотрите, какого качества можно добиться если использовать разные модели:\n",
        "* `DecisionTreeRegressor` из `sklearn`\n",
        "* `RandomForestRegressor` из `sklearn`\n",
        "* `CatBoostRegressor`\n",
        "\n",
        "Также вы можете попробовать линейные модели, другие бустинги (`LigthGBM` и `XGBoost`).\n",
        "\n",
        "Почти все библиотеки поддерживают удобный способ подбора гиперпараметров: посмотрите как это делать в [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) или в [catboost](https://catboost.ai/docs/concepts/python-reference_catboostregressor_grid_search.html).\n",
        "\n",
        "Проверяйте качество каждой модели на тестовой выборке и выберите наилучшую."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для начала попробуем избавиться от всех категориальных признаков и обучить модели **DecusionTreeRegressor** и **RandomForestRegressor**, которые по умолчанию не умеют работать с категормльными признаками, только на численных признаках"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучим модель с дефольтными параметрами и посмотрим на параметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depth of the tree trained on only numerical features: 48\n",
            "Number of leaves of the tree trained on only numerical features: 14250\n",
            "---------------------------------------\n",
            "Depth of the tree trained on numerical and categorical features: 45\n",
            "Number of leaves of the tree trained on numerical and categorical features: 14226\n",
            "---------------------------------------\n",
            "Depth of the tree trained on reduced numerical and categorical features: 47\n",
            "Number of leaves of the tree reducedtrained on numerical and categorical features: 14248\n"
          ]
        }
      ],
      "source": [
        "dt_num = DecisionTreeRegressor(random_state=42)\n",
        "dt_num.fit(X_train_num, y_train)\n",
        "print(f\"Depth of the tree trained on only numerical features: {dt_num.get_depth()}\")\n",
        "print(f\"Number of leaves of the tree trained on only numerical features: {dt_num.get_n_leaves()}\")\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "dt_1hot = DecisionTreeRegressor(random_state=42)\n",
        "dt_1hot.fit(X1hot_train, y1hot_train)\n",
        "print(f\"Depth of the tree trained on numerical and categorical features: {dt_1hot.get_depth()}\")\n",
        "print(f\"Number of leaves of the tree trained on numerical and categorical features: {dt_1hot.get_n_leaves()}\")\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "dt_1hot_red = DecisionTreeRegressor(random_state=42)\n",
        "dt_1hot_red.fit(X1hot_train_red, y1hot_train)\n",
        "print(f\"Depth of the tree trained on reduced numerical and categorical features: {dt_1hot_red.get_depth()}\")\n",
        "print(f\"Number of leaves of the tree reducedtrained on numerical and categorical features: {dt_1hot_red.get_n_leaves()}\")\n",
        "\n",
        "rf_1hot = RandomForestRegressor(random_state=42, n_jobs=3)\n",
        "rf_1hot.fit(X1hot_train, y1hot_train)\n",
        "\n",
        "cb_1hot = CatBoostRegressor(random_seed=42, thread_count=3, verbose=0)\n",
        "cb_1hot.fit(X1hot_train, y1hot_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of the prediction on the test set of the tree trained on only numerical features = 3722219.0\n",
            "RMSE of the prediction on the test set of the tree trained on both numerical and categorical features = 3780887.3\n",
            "RMSE of the prediction on the test set of the tree trained on reduced numerical and categorical features = 3856839.7\n",
            "RMSE of the prediction on the test set of the random forest = 3780887.3\n",
            "RMSE of the prediction on the test set of the catboost model = 2588089.7\n"
          ]
        }
      ],
      "source": [
        "y_pred_dt_num = dt_num.predict(X_test_num)\n",
        "y_pred_dt_1hot = dt_1hot.predict(X1hot_test)\n",
        "y_pred_dt_1hot_red = dt_1hot_red.predict(X1hot_test_red)\n",
        "y_pred_rf_1hot = rf_1hot.predict(X1hot_test)\n",
        "y_pred_cb_1hot = cb_1hot.predict(X1hot_test)\n",
        "\n",
        "rmse_dt_num = mean_squared_error(y_test, y_pred_dt_num, squared=False)\n",
        "rmse_dt_1hot = mean_squared_error(y_test, y_pred_dt_1hot, squared=False)\n",
        "rmse_dt_1hot_red = mean_squared_error(y_test, y_pred_dt_1hot_red, squared=False)\n",
        "rmse_rf_1hot = mean_squared_error(y_test, y_pred_rf_1hot, squared=False)\n",
        "rmse_cb_1hot = mean_squared_error(y_test, y_pred_cb_1hot, squared=False)\n",
        "print(f\"RMSE of the prediction on the test set of the tree trained on only numerical features = {rmse_dt_num:.1f}\")\n",
        "print(f\"RMSE of the prediction on the test set of the tree trained on both numerical and categorical features = {rmse_dt_1hot:.1f}\")\n",
        "print(f\"RMSE of the prediction on the test set of the tree trained on reduced numerical and categorical features = {rmse_dt_1hot_red:.1f}\")\n",
        "print(f\"RMSE of the prediction on the test set of the random forest = {rmse_dt_1hot:.1f}\")\n",
        "print(f\"RMSE of the prediction on the test set of the catboost model = {rmse_cb_1hot:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавление категориальных признаков через onehotencoding не улучшил метрику! А уменьшение размерности даже сильно ухудшил метрику. Скорее всего произошло переобучение. Подберем гиперпараметры модели. Начнем с дерева."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "parameters_dt = {\"max_depth\":[3, 5, 10, 13, 15],\n",
        "                \"max_features\":[\"auto\", \"sqrt\", \"log2\"],\n",
        "                \"min_samples_leaf\":[1, 5, 10, 15],\n",
        "                \"min_samples_split\":[2, 4, 6]}\n",
        "\n",
        "dt_param_search = GridSearchCV(dt, parameters_dt, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "dt_param_search.fit(X1hot_train_red, y1hot_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_depth': 10,\n",
              " 'max_features': 'auto',\n",
              " 'min_samples_leaf': 15,\n",
              " 'min_samples_split': 2}"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt_param_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Перейдем к слуайному лесу. Тут можно было бы больше параметров перебрать, но слишком уж долго считается. Здесь я не рассматриваю глубину дерева, потому что случайный робастен по отношению к глубине леса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n",
              "             param_grid={'max_depth': [6, 10, 15],\n",
              "                         'max_samples': [0.6666666666666666,\n",
              "                                         0.3333333333333333],\n",
              "                         'min_samples_leaf': [2, 5, 10],\n",
              "                         'n_estimators': [100, 200, 300],\n",
              "                         'warm_start': [True]},\n",
              "             scoring='neg_mean_squared_error')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf = RandomForestRegressor(random_state=42)\n",
        "parameters_rf = {\"n_estimators\":[100,200,300],\n",
        "                \"max_samples\":[2/3,1/3],\n",
        "                \"max_depth\":[6,10,15],\n",
        "                \"warm_start\":[True]}\n",
        "\n",
        "rf_param_search = GridSearchCV(rf, parameters_rf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "rf_param_search.fit(X1hot_train_red, y1hot_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_depth': 15,\n",
              " 'max_samples': 0.6666666666666666,\n",
              " 'min_samples_leaf': 2,\n",
              " 'n_estimators': 200,\n",
              " 'warm_start': True}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf_param_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В случае catboost тоже можно было поискать оптимальные гипермараметры в большем множестве, но время поиска тогда растянется на дни"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<catboost.core.CatBoostRegressor object at 0x7f91b54748d0>,\n",
              "             n_jobs=-1,\n",
              "             param_grid={'depth': [5, 7, 9],\n",
              "                         'iterations': [250, 500, 1000, 2000],\n",
              "                         'l2_leaf_reg': [3, 5, 10]},\n",
              "             scoring='neg_mean_squared_error')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters_cb = {\"iterations\":[250,500,1000,2000],\n",
        "                \"depth\":[5,7,9],\n",
        "                \"l2_leaf_reg\":[3,5,10]}\n",
        "\n",
        "cb = CatBoostRegressor(verbose=0, random_seed=42)\n",
        "\n",
        "grid_search_cb = GridSearchCV(cb, parameters_cb, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_cb.fit(X1hot_train_red, y1hot_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'depth': 7, 'iterations': 2000, 'l2_leaf_reg': 10}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search_cb.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучим лучшие модели на всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x7f9227991208>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt_best = DecisionTreeRegressor(max_depth=10, max_features='auto', \n",
        "                                splitter='best', min_samples_leaf=15, \n",
        "                                min_samples_split=2, random_state=42)\n",
        "dt_best.fit(X1hot_train_red, y1hot_train)\n",
        "\n",
        "rf_best = RandomForestRegressor(n_estimators=200, max_samples=2/3, \n",
        "                                min_samples_leaf=2, max_depth=15, random_state=42)\n",
        "rf_best.fit(X1hot_train_red, y1hot_train)\n",
        "\n",
        "cb_best = CatBoostRegressor(iterations=2000, depth=7, l2_leaf_reg=10)\n",
        "cb_best.fit(X1hot_train_red, y1hot_train, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сравним лучшие модели по метрике"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE decision tree: 3119856.37\n",
            "RMSE random forest: 2737697.43\n",
            "RMSE boosting: 2566334.15\n"
          ]
        }
      ],
      "source": [
        "y_pred_dt = dt_best.predict(X1hot_test_red)\n",
        "y_pred_rf = rf_best.predict(X1hot_test_red)\n",
        "y_pred_cb = cb_best.predict(X1hot_test_red)\n",
        "\n",
        "print(f\"RMSE decision tree: {mean_squared_error(y_test, y_pred_dt, squared=False):.2f}\")\n",
        "print(f\"RMSE random forest: {mean_squared_error(y_test, y_pred_rf, squared=False):.2f}\")\n",
        "print(f\"RMSE boosting: {mean_squared_error(y_test, y_pred_cb, squared=False):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Победитель: catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxQnozbtvD6x"
      },
      "source": [
        "### Ensemble v.1 (3 балла)\n",
        "\n",
        "Ансамбли иногда оказываются лучше чем одна большая модель.\n",
        "\n",
        "В колонке `product_type` содержится информация о том, каким является объявление: `Investment` (продажа квартиры как инвестиции) или `OwnerOccupier` (продажа квартиры для жилья). Логично предположить, что если сделать по модели на каждый из этих типов, то качество будет выше.\n",
        "\n",
        "Обучите свои лучшие модели на отдельно на `Investment` и `OwnerOccupier` (т.е. у вас будет `model_invest`, обученная на `(invest_train_X, invest_train_Y)` и `model_owner`, обученная на `(owner_train_X, owner_train_Y)`) и проверьте качество на отложенной выборке (т.е. на исходном `test_split`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Разделим данные по признаку product_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "invest_df = df_1hot[df_1hot['product_type_Investment']==1]\n",
        "owner_df = df_1hot[df_1hot['product_type_OwnerOccupier']==1]\n",
        "\n",
        "invest_df = invest_df.drop(['product_type_Investment', 'product_type_OwnerOccupier'], axis=1)\n",
        "owner_df = owner_df.drop(['product_type_Investment', 'product_type_OwnerOccupier'], axis=1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xinvest = invest_df.drop('price', axis=1)\n",
        "yinvest = invest_df['price']\n",
        "\n",
        "Xowner = owner_df.drop('price', axis=1)\n",
        "yowner = owner_df['price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Разделим на тренировочную и тестовые выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HeJOCYa-O_VE"
      },
      "outputs": [],
      "source": [
        "Xinvest_train, Xinvest_test, yinvest_train, yinvest_test = train_test_split(Xinvest, yinvest, test_size=0.2, shuffle=False)\n",
        "Xowner_train, Xowner_test, yowner_train, yowner_test = train_test_split(Xowner, yowner, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Заменим nan на медианное значение признака"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "invest_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "owner_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "\n",
        "Xinvest_train = invest_imputer.fit_transform(Xinvest_train)\n",
        "Xinvest_test = invest_imputer.transform(Xinvest_test)\n",
        "\n",
        "Xowner_train = owner_imputer.fit_transform(Xowner_train)\n",
        "Xowner_test = owner_imputer.transform(Xowner_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Уменьшим размерность данных (количество признаков)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "etr_invest = ExtraTreesRegressor(n_estimators=50, random_state=42)\n",
        "etr_invest.fit(Xinvest_train, yinvest_train)\n",
        "\n",
        "selector_invest = SelectFromModel(etr_invest, prefit=True)\n",
        "Xinvest_train_red = selector_invest.transform(Xinvest_train)\n",
        "Xinvest_test_red = selector_invest.transform(Xinvest_test)\n",
        "\n",
        "etr_owner = ExtraTreesRegressor(n_estimators=50, random_state=42)\n",
        "etr_owner.fit(Xowner_train, yowner_train)\n",
        "\n",
        "selector_owner = SelectFromModel(etr_invest, prefit=True)\n",
        "Xowner_train_red = selector_owner.transform(Xowner_train)\n",
        "Xowner_test_red = selector_owner.transform(Xowner_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучим лучшую модель на новых данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE Investment = 2905520.65\n",
            "RMSE Owner = 1939059.53\n"
          ]
        }
      ],
      "source": [
        "model_invest = CatBoostRegressor(iterations=2000, depth=7, l2_leaf_reg=10, verbose=0)\n",
        "model_invest.fit(Xinvest_train_red, yinvest_train)\n",
        "\n",
        "model_owner = CatBoostRegressor(iterations=2000, depth=7, l2_leaf_reg=10, verbose=0)\n",
        "model_owner.fit(Xowner_train_red, yowner_train)\n",
        "\n",
        "yinvest_pred = model_invest.predict(Xinvest_test_red)\n",
        "yowner_pred = model_owner.predict(Xowner_test_red)\n",
        "\n",
        "print(f\"RMSE Investment = {mean_squared_error(yinvest_test, yinvest_pred, squared=False):.2f}\")\n",
        "print(f\"RMSE Owner = {mean_squared_error(yowner_test, yowner_pred, squared=False):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Модель намного лучше предсказывает target для квартир, которые продаются для жилья."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8t-klaK2A6s"
      },
      "source": [
        "### (*) Ensemble v.2 (дополнительно, 2 балла)\n",
        "\n",
        "Попробуйте сделать для `Investment` более сложную модель: обучите `CatBoostRegressor` и `HuberRegressor` из `sklearn`, а затем сложите их предсказания с весами `w_1` и `w_2` (выберите веса сами; сумма весов равняется 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xinvest_train2, Xinvest_val, yinvest_train2, yinvest_val = train_test_split(Xinvest_train_red, yinvest_train, test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JOKD-l8HuOgM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HuberRegressor(max_iter=2000.0)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_invest_cb = CatBoostRegressor(iterations=2000, depth=7, l2_leaf_reg=10, verbose=0)\n",
        "model_invest_cb.fit(Xinvest_train2, yinvest_train2)\n",
        "\n",
        "model_invest_huber = HuberRegressor(max_iter=2e+3)\n",
        "model_invest_huber.fit(Xinvest_train2, yinvest_train2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создадим сетку весов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = np.linspace(0, 1, 15)\n",
        "w = zip(w1, 1-w1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best w1 = 0.9286, w2 = 0.0714\n"
          ]
        }
      ],
      "source": [
        "rmse_invest = []\n",
        "for w1, w2 in w:\n",
        "    y_pred1 = model_invest_cb.predict(Xinvest_val)\n",
        "    y_pred2 = model_invest_huber.predict(Xinvest_val)\n",
        "    y_pred = w1*y_pred1 + w2*y_pred2\n",
        "    rmse_invest.append(tuple((w1, w2, mean_squared_error(yinvest_val, y_pred, squared=False))))\n",
        "\n",
        "best_w1, best_w2, _ = min(rmse_invest, key=lambda x: x[2])\n",
        "print(f\"Best w1 = {best_w1:.4f}, w2 = {best_w2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE Investment = 2857284.51\n"
          ]
        }
      ],
      "source": [
        "yinvest_pred = best_w1*model_invest_cb.predict(Xinvest_test_red) + \\\n",
        "                best_w2*model_invest_huber.predict(Xinvest_test_red)\n",
        "\n",
        "print(f\"RMSE Investment = {mean_squared_error(yinvest_test, yinvest_pred, squared=False):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Более сложная модель: комбинация `CatboostRegressor` и `HuberRegressor` смогла незначительно улучшить метрику на тестовой выборке"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Task.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
